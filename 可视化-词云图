import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from docx import Document
from PIL import Image
import numpy as np

# 定义停用词列表
stopwords = set(["的", "能够","是","等", "了", "和", "在", "有", "不", "人", "都", "也", "很", "到", "说", "要", "会", "去", "你", "会", "没有", "看", "是", "这"])

# 从Word文档中读取文本
def read_text_from_docx(file_path):
    doc = Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return "\n".join(full_text)

# 读取D盘中的Word文档内容
file_path = "D:/食疗.docx"
collected_text = read_text_from_docx(file_path)

# 打印读取到的文本内容（调试用）
print("从文档中读取的文本内容：")
print(collected_text)

# 使用jieba进行中文分词，并过滤停用词
words = jieba.cut(collected_text)
filtered_words = [word for word in words if word not in stopwords]  # 过滤停用词
word_list = " ".join(filtered_words)

# 打印过滤后的分词结果（调试用）
print("过滤停用词后的分词结果：")
print(word_list)

# 加载自定义图片作为词云形状
mask_image_path = "D:/食疗.png"  # 自定义图片路径
mask_image = np.array(Image.open(mask_image_path))

# 生成词云
wordcloud = WordCloud(
    font_path="simhei.ttf",  # 设置字体路径，确保支持中文
    mask=mask_image,         # 使用自定义图片作为词云形状
    background_color="white"  # 设置词云背景颜色
).generate(word_list)

# 显示词云图
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")  # 关闭坐标轴
plt.show()

# 保存词云图到本地
wordcloud.to_file("D:/食疗词云图.png")  # 保存到D盘
print("词云图已保存到D盘！")
